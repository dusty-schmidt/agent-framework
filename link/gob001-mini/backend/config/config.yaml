# gob-001-mini - Configuration Template
# Copy this file as 'config.yaml' in this directory and customize as needed

# API Configuration
api:
  openrouter_base_url: "https://openrouter.ai/api/v1"
  google_ai_studio_base_url: "https://generativelanguage.googleapis.com/v1beta"
  default_max_tokens: 1000
  default_temperature: 0.7

# Model Configuration
models:
  # Model tiers for different use cases - All using OpenRouter for testing
  tiers:
    utility: "openai/gpt-oss-20b"
    chat: "openai/gpt-oss-20b"
    premium: "openai/gpt-oss-20b"
    specialized: "openai/gpt-oss-20b"

  # Main agent (primary orchestrator)
  main:
    model: "openai/gpt-oss-20b"
    temperature: 0.7
    max_tokens: 1500

  # Persona-specific models
  personas:
    developer:
      model: "openai/gpt-oss-20b"
      temperature: 0.2 # Lower temperature for precise code generation
      max_tokens: 2000
    creative:
      model: "openai/gpt-oss-20b"
      temperature: 0.8 # Higher temperature for creative tasks
      max_tokens: 1500

  # Universal agents available to all personas
  universal:
    utility:
      model: "openai/gpt-oss-20b"
      temperature: 0.5
      max_tokens: 1000
    web_browser:
      model: "gemini-2.0-flash-lite"
      temperature: 0.3
      max_tokens: 1500
    embedding:
      model: "text-embedding-ada-002"

  # Google AI Studio models (alternative provider)
  google:
    main:
      model: "gemini-pro"
      temperature: 0.7
      max_tokens: 1500
    utility:
      model: "gemini-pro"
      temperature: 0.5
      max_tokens: 1000
    coding:
      model: "gemini-pro"
      temperature: 0.2
      max_tokens: 2000

  # Fallback models (using same Google model for consistency during testing)
  fallbacks:
    main_fallback:
      model: "gemini-2.0-flash-lite"
      temperature: 0.7
      max_tokens: 1500
    utility_fallback:
      model: "gemini-2.0-flash-lite"
      temperature: 0.5
      max_tokens: 1000
    developer_fallback:
      model: "gemini-2.0-flash-lite"
      temperature: 0.2
      max_tokens: 2000
    creative_fallback:
      model: "gemini-2.0-flash-lite"
      temperature: 0.8
      max_tokens: 1500

# Agent Configuration
agents:
  # Main orchestrator agent
  orchestrator:
    temperature: 0.3
    max_tokens: 500
    routing_confidence_threshold: 0.3

  # Specialized agents
  coding_assistant:
    temperature: 0.2
    max_tokens: 2000

  general_assistant:
    temperature: 0.7
    max_tokens: 1000

  # Top-level routing configuration
  top_level:
    persona_selection_threshold: 0.3
    routing_confidence_threshold: 0.4

  # Persona configurations
  personas:
    developer:
      available_helpers: ["coding_assistant", "web_browser"]
      helper_threshold: 0.5
    creative:
      available_helpers: ["general_assistant"]
      helper_threshold: 0.6

  # Fallback system configuration
  fallback:
    enabled: true
    max_retries: 2
    retry_delay: 1.0
    fallback_notification: true

# System Configuration
system:
  max_conversation_history: 20
  max_session_memory: 20
  log_level: "INFO" # DEBUG, INFO, WARNING, ERROR
  log_agent_routing: true
  log_model_usage: true
  enable_streaming: false
  concurrent_requests: 10
