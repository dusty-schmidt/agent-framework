# models.toml

[[endpoint]]
url = "https://openrouter.ai/api/v1/chat"
provider = "OpenRouter"

  [[endpoint.model]]
  name = "GPT-4o"
  version = "2024-07"
  context_window = 128000
  input_limit = 32768
  output_limit = 4096
  cost = "0.005 USD / 1K tokens"
  latency = "low"
  throughput = "high"
  modes = ["text", "vision", "speech"]
  temperature_default = 0.7

  [[endpoint.model]]
  name = "Claude 3 Opus"
  version = "2024-06"
  context_window = 200000
  input_limit = 100000
  output_limit = 4096
  cost = "0.015 USD / 1K tokens"
  latency = "medium"
  throughput = "medium"
  modes = ["text", "vision"]
  temperature_default = 0.5

[[endpoint]]
url = "http://localhost:8002/v1/chat"
provider = "Local"

  [[endpoint.model]]
  name = "Llama Maverick"
  version = "1.2.0"
  context_window = 32000
  input_limit = 8192
  output_limit = 2048
  cost = "free"
  latency = "variable"
  throughput = "depends on hardware"
  modes = ["text"]
  temperature_default = 0.8
